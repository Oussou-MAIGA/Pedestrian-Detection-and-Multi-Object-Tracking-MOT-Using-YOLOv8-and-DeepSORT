---

# ðŸ›  Technologies utilisÃ©es

<p align="left">

  <!-- Python -->
  <img src="https://img.shields.io/badge/Python-3.11-blue?logo=python" height="28"/>

  <!-- Ultralytics -->
  <img src="https://img.shields.io/badge/Ultralytics-YOLOv8-brightgreen?logo=ultralytics" height="28"/>

  <!-- OpenCV -->
  <img src="https://img.shields.io/badge/OpenCV-4.x-red?logo=opencv" height="28"/>

  <!-- Scikit-learn -->
  <img src="https://img.shields.io/badge/Scikit--Learn-SVM-orange?logo=scikitlearn" height="28"/>

  <!-- PyWavelets -->
  <img src="https://img.shields.io/badge/PyWavelets-Filtering-yellow" height="28"/>

  <!-- DeepSORT -->
  <img src="https://img.shields.io/badge/DeepSORT-ReID%20MobileNet-purple" height="28"/>

  <!-- ByteTrack -->
  <img src="https://img.shields.io/badge/ByteTrack-MOT-blueviolet" height="28"/>

  <!-- SLURM -->
  <img src="https://img.shields.io/badge/SLURM-HPC%20Cluster-green?logo=linux" height="28"/>

  <!-- GPU -->
  <img src="https://img.shields.io/badge/NVIDIA-H100%20GPU-76B900?logo=nvidia&logoColor=white" height="28"/>

</p>

---

# DÃ©tection et Suivi de PiÃ©tons  
**Haar/SVM Â· HOG/SVM Â· YOLOv8s Â· DeepSORT (ReID MobileNet) Â· ByteTrack**

Projet du cours â€” UniversitÃ© de Moncton  
Auteur : **Ousmane Maiga**  
Superviseur : **Pr. Moulay Akhloufi â€“ PRIME Lab**

---

# 1. Description du projet

Ce projet compare trois approches de **dÃ©tection de piÃ©tons** :

- Haar + SVM  
- HOG + SVM  
- YOLOv8s (meilleur modÃ¨le)

et deux mÃ©thodes de **suivi multi-objets** :

- DeepSORT (avec ReID MobileNet)  
- ByteTrack (implÃ©mentation Ultralytics)

Objectifs :

- analyser pourquoi les dÃ©tecteurs classiques Ã©chouent en scÃ¨ne rÃ©elle  
- Ã©tudier la gÃ©nÃ©ralisation cross-dataset (**Caltech â†’ INRIA**)  
- mesurer lâ€™impact de la qualitÃ© des dÃ©tections sur le tracking  
- produire des rÃ©sultats visuels et deux vidÃ©os finales de suivi

---

# 2. Structure du projet

```text
projet_detection_suivi_pietons/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ train_yolo.slurm
â”‚
â”œâ”€â”€ images/                 # rÃ©sultats de dÃ©tection pour le rapport / README
â”‚   â”œâ”€â”€ haar_caltech_1.png
â”‚   â”œâ”€â”€ haar_caltech_2.png
â”‚   â”œâ”€â”€ haar_inria_1.png
â”‚   â”œâ”€â”€ haar_inria_2.png
â”‚   â”œâ”€â”€ hog_inria_1.png
â”‚   â”œâ”€â”€ hog_inria_2.png
â”‚   â”œâ”€â”€ yolo_caltech_inria_1.jpg
â”‚   â””â”€â”€ yolo_caltech_inria_2.jpg
â”‚
â”œâ”€â”€ videos/                 # rÃ©sultats de suivi (DeepSORT / ByteTrack)
â”‚   â”œâ”€â”€ DeepSort.mp4
â”‚   â””â”€â”€ ByteTrack.mp4
â”‚
â”œâ”€â”€ modeles/
â”‚   â””â”€â”€ caltech_person/
â”‚       â””â”€â”€ weights/
â”‚           â””â”€â”€ best.pt    # meilleur modÃ¨le YOLOv8s (entraÃ®nÃ© sur Caltech)
â”‚
â”œâ”€â”€ datasets/               # Ã  remplir via les liens officiels (Section 3)
â”‚   â”œâ”€â”€ Caltech/
â”‚   â”œâ”€â”€ INRIA/
â”‚   â””â”€â”€ KITTI/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ feature_haar_inria.py
â”‚   â”œâ”€â”€ features_hog_inria.py
â”‚   â”œâ”€â”€ patch_and_negatifs_inria.py
â”‚   â”œâ”€â”€ entrainement_svm_inria.py
â”‚   â”œâ”€â”€ entrainement_svm_hog_inria.py
â”‚   â”œâ”€â”€ detect_inria_svm.py
â”‚   â”œâ”€â”€ detect_inria_hog_svm.py
â”‚   â”œâ”€â”€ track_ReID_deepsort.py
â”‚   â”œâ”€â”€ eval_MOT.py
â”‚   â”œâ”€â”€ convert_Pred_to_MOT.py
â”‚   â”œâ”€â”€ convert_kitti_GT_to_MOT.py
â”‚   â”œâ”€â”€ extract_images.py
â”‚   â”œâ”€â”€ extract_annotations.py
â”‚   â”œâ”€â”€ convertir_vbb.py
â”‚   â”œâ”€â”€ video_to_frames.py
â”‚   â””â”€â”€ images_to_videos.py
â”‚
â””â”€â”€ config/
    â”œâ”€â”€ data_caltech.yaml
    â”œâ”€â”€ data_inria.yaml
    â””â”€â”€ liste_chemin_image.sh


```

# 3. Datasets (liens officiels)
Les datasets sont trop volumineux pour Ãªtre versionnÃ©s.
Ils doivent Ãªtre tÃ©lÃ©chargÃ©s depuis les sites officiels puis placÃ©s dans datasets/.

ðŸ”¹ Caltech Pedestrian
Site : https://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/

Dossier cible : datasets/Caltech/

Les scripts convertir_vbb.py, extract_images.py, extract_annotations.py
permettent de convertir .seq + .vbb â†’ images + labels YOLO.

ðŸ”¹ INRIA Person
Repo : https://github.com/olt/inria-object-detection

Dossier cible : datasets/INRIA/

ðŸ”¹ KITTI Tracking 
Site : https://www.cvlibs.net/datasets/kitti/eval_tracking.php

Dossier cible : datasets/KITTI/

Une fois les archives KITTI extraites, vous obtenez la structure officielle, par exemple :

datasets/KITTI/
 â””â”€â”€ tracking/
     â””â”€â”€ training/
         â””â”€â”€ image_02/
             â”œâ”€â”€ 0000/
             â”œâ”€â”€ 0001/
             â”œâ”€â”€ 0012/
             â”œâ”€â”€ 0019/
             â””â”€â”€ ...

Lâ€™idÃ©e est simplement de placer les images de tracking dans datasets/KITTI/...
en respectant lâ€™organisation native de KITTI.

# 4. RÃ©sultats de dÃ©tection

Haar + SVM (Caltech / INRIA)
<p align="center"> <img src="images/haar_caltech_1.png" width="260"/> <img src="images/haar_caltech_2.png" width="260"/> </p> <p align="center"> <img src="images/haar_inria_1.png" width="260"/> <img src="images/haar_inria_2.png" width="260"/> </p>
HOG + SVM (INRIA)
<p align="center"> <img src="images/hog_inria_1.png" width="260"/> <img src="images/hog_inria_2.png" width="260"/> </p>
YOLOv8s (modÃ¨le entraÃ®nÃ© sur Caltech, testÃ© sur INRIA)
<p align="center"> <img src="images/yolo_caltech_inria_1.jpg" width="260"/> <img src="images/yolo_caltech_inria_2.jpg" width="260"/> </p>

# 5. RÃ©sultats de suivi
Les vidÃ©os finales de suivi sont dans :

videos/DeepSort.mp4

videos/ByteTrack.mp4

DeepSORT
Voir la vidÃ©o DeepSORT

ByteTrack
Voir la vidÃ©o ByteTrack

# 6. Environnement logiciel (Cluster Trilium)
Sur le cluster Trilium, avant dâ€™exÃ©cuter lâ€™entraÃ®nement ou les Ã©valuations YOLO,
les modules et bibliothÃ¨ques suivants sont chargÃ©s / installÃ©s :

module load python/3.11.5
module load gcc opencv/4.12.0 python script-stick

# activation de l'environnement virtuel (exemple)
source /chemin/vers/mon_env/bin/activate

# installation des dÃ©pendances
pip install --no-index \
  -f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic \
  pywavelets scikit-learn ultralytics

Ces commandes sont exÃ©cutÃ©es avant :

sbatch train_yolo.slurm

yolo detect val ...

yolo detect predict ...

yolo track ...

# 7. ModÃ¨le YOLOv8s (base + fine-tuning)
## 7.1 ModÃ¨le de base (prÃ©-entraÃ®nÃ© COCO)
Fichier : yolov8s.pt

TÃ©lÃ©chargement officiel :
https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt

Ce modÃ¨le est utilisÃ© dans train_yolo.slurm comme point de dÃ©part :

MODEL="yolov8s.pt"

## 7.2 ModÃ¨le final (fine-tunÃ© sur Caltech)
Le fine-tuning sur Caltech produit le meilleur modÃ¨le du projet (celui utilisÃ© dans lâ€™article) :

modeles/caltech_person/weights/best.pt
Ce modÃ¨le :

est entraÃ®nÃ© sur Caltech (train/val),

est Ã©valuÃ© automatiquement sur Caltech (test) dans le script SLURM (yolo val split=test),

est ensuite rÃ©utilisÃ© pour lâ€™Ã©valuation Caltech â†’ INRIA (cross-dataset),

sert de modÃ¨le unique pour tous les tests et pour les deux trackers (DeepSORT et ByteTrack).

# 8. EntraÃ®nement YOLOv8s sur Caltech
Lâ€™entraÃ®nement se fait via le script SLURM :

sbatch train_yolo.slurm
Dans ce script :

model=$MODEL pointe vers yolov8s.pt (modÃ¨le de base COCO),

data=config/data_caltech.yaml dÃ©crit les chemins du dataset Caltech converti en format YOLO,

les paramÃ¨tres par dÃ©faut (batch, epochs, etc.) sont ajustÃ©s pour le cluster.

Ã€ la fin de lâ€™entraÃ®nement, Ultralytics valide automatiquement sur le split test de Caltech :

yolo detect val \
  model=modeles/caltech_person/weights/best.pt \
  data=config/data_caltech.yaml \
  split=test

Ce yolo val fournit les performances officielles Caltech â†’ Caltech
utilisÃ©es dans le rapport (mAP@50, F1, etc.).

Temps dâ€™exÃ©cution observÃ© sur Trilium :

~ 2 h 05 min 41 s sur 4 GPUs (H100).

# 9. Ã‰valuation YOLOv8s (cross-dataset Caltech â†’ INRIA)
AprÃ¨s lâ€™entraÃ®nement sur Caltech, on rÃ©utilise le mÃªme modÃ¨le :

yolo detect val \
  model=modeles/caltech_person/weights/best.pt \
  data=config/data_inria.yaml \
  split=test

RÃ©sultat principal (Caltech â†’ INRIA) :

mAP@50 â‰ˆ 0.689

F1 et PR dÃ©taillÃ©s dans lâ€™article (courbes PR/F1 + matrice de confusion).

Dans le rapport, câ€™est ce cas Caltech â†’ INRIA qui est considÃ©rÃ© comme
meilleur scÃ©nario global (modÃ¨le entraÃ®nÃ© sur un dataset plus difficile et testÃ© sur un plus simple).

# 10. Suivi multi-objets
## 10.1 DeepSORT (ReID MobileNet)

DeepSORT nâ€™est pas intÃ©grÃ© directement dans Ultralytics :
on utilise le script Python track_ReID_deepsort.py, qui prend en entrÃ©e :

les images KITTI pour une sÃ©quence (ex. 0019),

les dÃ©tections YOLOv8s au format e (.txt) gÃ©nÃ©rÃ©es par Ultralytics,

un dossier de sortie pour les frames annotÃ©es et les labels avec ID.

### 10.1.1 GÃ©nÃ©rer les dÃ©tections YOLO sur KITTI

yolo detect predict \
  model=modeles/caltech_person/weights/best.pt \
  source=datasets/KITTI/tracking/training/image_02/0019 \
  imgsz=1408 \
  conf=0.60 \
  save=True \
  save_txt=True \
  project=runs/detect \
  name=kitti_0019_yolo

Cela produit une structure de ce type :

runs/detect/kitti_0019_yolo/
 â”œâ”€â”€ 000000.png
 â”œâ”€â”€ 000001.png
 â”œâ”€â”€ ...
 â””â”€â”€ labels/
      â”œâ”€â”€ 000000.txt    # cls cx cy w h conf
      â”œâ”€â”€ 000001.txt
      â””â”€â”€ ...
### 10.1.2 Lancer DeepSORT


python scripts/track_ReID_deepsort.py \
  --img_dir  datasets/KITTI/tracking/training/image_02/0019 \
  --dets_dir runs/detect/kitti_0019_yolo/labels \
  --out_dir  runs/tracking/deepsort_0019 \
  --embedder mobilenet \
  --max_age 10 \
  --n_init 3 \
  --max_cosine_distance 0.4

ParamÃ¨tres principaux :

--img_dir : images KITTI dâ€™une sÃ©quence (ex. 0019)

--dets_dir : fichiers .txt YOLO gÃ©nÃ©rÃ©s par yolo detect predict

--out_dir : dossier de sortie des rÃ©sultats DeepSORT

--embedder : modÃ¨le ReID utilisÃ© (mobilenet)

--max_age : durÃ©e de vie dâ€™une piste sans dÃ©tection

--n_init : nombre de frames nÃ©cessaires pour valider une piste

--max_cosine_distance : seuil dâ€™acceptation pour la similaritÃ© dâ€™apparence

RÃ©sultats :

runs/tracking/deepsort_0019/
 â”œâ”€â”€ frames/
 â”‚    â”œâ”€â”€ 000000.png      # image annotÃ©e (bbox + ID)
 â”‚    â”œâ”€â”€ 000001.png
 â”‚    â””â”€â”€ ...
 â””â”€â”€ labels/
      â”œâ”€â”€ 000000.txt      # cls cx cy w h track_id
      â”œâ”€â”€ 000001.txt
      â””â”€â”€ ...
Les vidÃ©os finales visibles dans videos/DeepSort.mp4 sont construites
Ã  partir de ces frames via images_to_videos.py.

## 10.2 ByteTrack (Ultralytics)
ByteTrack est directement intÃ©grÃ© dans Ultralytics via yolo track.

Commande dâ€™exemple (sÃ©quence KITTI 0019)

yolo track \
  model="modeles/caltech_person/weights/best.pt" \
  source="datasets/KITTI/tracking/training/image_02/0019" \
  imgsz=1408 \
  conf=0.60 \
  tracker="bytetrack.yaml" \
  save=True \
  save_txt=True \
  save_json=True \
  project="runs/kitti_eval" \
  name="bytetrack_0019"

model= : modÃ¨le YOLOv8s fine-tunÃ© sur Caltech

source= : dossier dâ€™images KITTI pour une sÃ©quence

tracker="bytetrack.yaml" : active ByteTrack

save=True : enregistre la vidÃ©o annotÃ©e (.mp4)

save_txt=True : enregistre les labels avec track_id

save_json=True : exporte les rÃ©sultats en JSON (format MOT-compatible)

Sorties typiques :

runs/kitti_eval/bytetrack_0019/
 â”œâ”€â”€ bytetrack_0019.mp4        # vidÃ©o annotÃ©e
 â”œâ”€â”€ labels/
 â”‚    â”œâ”€â”€ 000000.txt           # cls cx cy w h track_id
 â”‚    â”œâ”€â”€ 000001.txt
 â”‚    â””â”€â”€ ...
 â””â”€â”€ predictions.json          # rÃ©sultats pour Ã©valuation MOT

Ces fichiers peuvent ensuite Ãªtre convertis et Ã©valuÃ©s avec :

scripts/convert_Pred_to_MOT.py

scripts/eval_MOT.py

pour obtenir les mÃ©triques IDF1, MOTA, etc., comme dans lâ€™article.

# 11. ReproductibilitÃ© (rÃ©sumÃ©)
Charger lâ€™environnement Trilium (Section 6)

TÃ©lÃ©charger et placer les datasets (Section 3)

Convertir Caltech en images + YOLO (scripts convertir_vbb.py, extract_images.py, extract_annotations.py)

GÃ©nÃ©rer les splits :

 config/liste_chemin_image.sh

EntraÃ®ner YOLOv8s sur Caltech :

sbatch train_yolo.slurm

â†’ modÃ¨le : modeles/caltech_person/weights/best.pt
â†’ validation automatique Caltech â†’ Caltech (yolo val split=test)

Ã‰valuer Caltech â†’ INRIA :

yolo detect val \
  model=modeles/caltech_person/weights/best.pt \
  data=config/data_inria.yaml \
  split=test

GÃ©nÃ©rer les dÃ©tections KITTI (pour le tracking) avec yolo detect predict.

Lancer DeepSORT avec track_ReID_deepsort.py.

Lancer ByteTrack avec yolo track ... tracker="bytetrack.yaml".

# 12. ModÃ¨le final du projet
Le modÃ¨le unique utilisÃ© dans tous les rÃ©sultats de lâ€™article est :

modeles/caltech_person/weights/best.pt
entraÃ®nÃ© sur Caltech

Ã©valuÃ© sur Caltech (officiel) via yolo val split=test

testÃ© en cross-dataset Caltech â†’ INRIA (meilleure configuration)

utilisÃ© pour DeepSORT et ByteTrack sur KITTI.

::contentReference[oaicite:0]{index=0}
---

# Contact

Pour toute question concernant le projet, vous pouvez contacter :

**Ousmane Maiga**  
**eom6713@umoncton.ca**

