# TL;DR â€” ExÃ©cuter le projet en quelques temps

git clone [<URL_DU_REPO>](https://github.com/Oussou-MAIGA/Pedestrian-Detection-and-Multi-Object-Tracking-MOT-Using-YOLOv8-and-DeepSORT.git)

cd Pedestrian-Detection-and-Multi-Object-Tracking-MOT-Using-YOLOv8-and-DeepSORT


# Installer les dÃ©pendances (pour installation locale)
pip install -r requirements.txt

# TÃ©lÃ©charger les datasets (Section 3)
# puis entraÃ®ner YOLO :
yolo detect train model=yolov8s.pt data=config/data_caltech.yaml

# Ã‰valuer Caltech â†’ INRIA :
yolo detect val model=modeles/caltech_person/weights/best.pt data=config/data_inria.yaml

# Lancer DeepSORT :
python scripts/track_ReID_deepsort.py --img_dir ... --dets_dir ...

# Lancer ByteTrack :
yolo track model=modeles/caltech_person/weights/best.pt tracker=bytetrack.yaml source=...


---

# Technologies utilisÃ©es

<p align="left">

  <!-- Python -->
  <img src="https://img.shields.io/badge/Python-3.11-blue?logo=python" height="28"/>

  <!-- Ultralytics -->
  <img src="https://img.shields.io/badge/Ultralytics-YOLOv8-brightgreen?logo=ultralytics" height="28"/>

  <!-- OpenCV -->
  <img src="https://img.shields.io/badge/OpenCV-4.x-red?logo=opencv" height="28"/>

  <!-- Scikit-learn -->
  <img src="https://img.shields.io/badge/Scikit--Learn-SVM-orange?logo=scikitlearn" height="28"/>

  <!-- PyWavelets -->
  <img src="https://img.shields.io/badge/PyWavelets-Filtering-yellow" height="28"/>

  <!-- DeepSORT -->
  <img src="https://img.shields.io/badge/DeepSORT-ReID%20MobileNet-purple" height="28"/>

  <!-- ByteTrack -->
  <img src="https://img.shields.io/badge/ByteTrack-MOT-blueviolet" height="28"/>

  <!-- SLURM -->
  <img src="https://img.shields.io/badge/SLURM-HPC%20Cluster-green?logo=linux" height="28"/>

  <!-- GPU -->
  <img src="https://img.shields.io/badge/NVIDIA-H100%20GPU-76B900?logo=nvidia&logoColor=white" height="28"/>

</p>

---

# DÃ©tection et Suivi de PiÃ©tons  
**Haar/SVM Â· HOG/SVM Â· YOLOv8s Â· DeepSORT (ReID MobileNet) Â· ByteTrack**

Projet du cours â€” UniversitÃ© de Moncton  
Auteur : **Ousmane MAIGA**  
Superviseur : **Pr. Moulay AKHLOUFI â€“ PRIME Lab**

---

# 1. Description du projet

<p align="center">
  <img src="images/pipeline_detection_tracking.png" width="750">
</p>


Ce projet compare trois approches de **dÃ©tection de piÃ©tons** :

- Haar + SVM  
- HOG + SVM  
- YOLOv8s (meilleur modÃ¨le)

et deux mÃ©thodes de **suivi multi-objets** :

- DeepSORT (avec ReID MobileNet)  
- ByteTrack (implÃ©mentation Ultralytics)

Objectifs :

- analyser pourquoi les dÃ©tecteurs classiques Ã©chouent en scÃ¨ne rÃ©elle  
- Ã©tudier la gÃ©nÃ©ralisation cross-dataset (**Caltech â†’ INRIA**)  
- mesurer lâ€™impact de la qualitÃ© des dÃ©tections sur le tracking  
- produire des rÃ©sultats visuels et deux vidÃ©os finales de suivi

---

# 2. Structure du projet

```text
projet_detection_suivi_pietons/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ train_yolo.slurm
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ images/                 # rÃ©sultats de dÃ©tection pour le rapport / README
â”‚   â”œâ”€â”€ haar_caltech_1.png
â”‚   â”œâ”€â”€ haar_caltech_2.png
â”‚   â”œâ”€â”€ haar_inria_1.png
â”‚   â”œâ”€â”€ haar_inria_2.png
â”‚   â”œâ”€â”€ hog_inria_1.png
â”‚   â”œâ”€â”€ hog_inria_2.png
â”‚   â”œâ”€â”€ yolo_caltech_inria_1.jpg
â”‚   â””â”€â”€ yolo_caltech_inria_2.jpg
â”‚
â”œâ”€â”€ videos/                 # rÃ©sultats de suivi (DeepSORT / ByteTrack)
â”‚   â”œâ”€â”€ DeepSort.mp4
â”‚   â””â”€â”€ ByteTrack.mp4
â”‚
â”œâ”€â”€ modeles/
â”‚   â””â”€â”€ caltech_person/
â”‚       â””â”€â”€ weights/
â”‚           â””â”€â”€ best.pt    # meilleur modÃ¨le YOLOv8s (entraÃ®nÃ© sur Caltech)
â”‚
â”œâ”€â”€ datasets/               # Ã  remplir via les liens officiels (Section 3)
â”‚   â”œâ”€â”€ Caltech/
â”‚   â”œâ”€â”€ INRIA/
â”‚   â””â”€â”€ kitti_tracking/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ feature_haar_inria.py
â”‚   â”œâ”€â”€ features_hog_inria.py
â”‚   â”œâ”€â”€ patch_and_negatifs_inria.py
â”‚   â”œâ”€â”€ entrainement_svm_inria.py
â”‚   â”œâ”€â”€ entrainement_svm_hog_inria.py
â”‚   â”œâ”€â”€ detect_inria_svm.py
â”‚   â”œâ”€â”€ detect_inria_hog_svm.py
â”‚   â”œâ”€â”€ track_ReID_deepsort.py
â”‚   â”œâ”€â”€ eval_MOT.py
â”‚   â”œâ”€â”€ convert_Pred_to_MOT.py
â”‚   â”œâ”€â”€ convert_kitti_GT_to_MOT.py
â”‚   â”œâ”€â”€ extract_images.py
â”‚   â”œâ”€â”€ extract_annotations.py
â”‚   â”œâ”€â”€ convertir_vbb.py
â”‚   â”œâ”€â”€ video_to_frames.py
â”‚   â””â”€â”€ images_to_videos.py
â”‚
â””â”€â”€ config/
    â”œâ”€â”€ data_caltech.yaml
    â”œâ”€â”€ data_inria.yaml
    â””â”€â”€ liste_chemin_image.sh


```

# 3. Datasets (liens officiels)
Les datasets sont trop volumineux pour Ãªtre versionnÃ©s.
Ils doivent Ãªtre tÃ©lÃ©chargÃ©s depuis les sites officiels puis placÃ©s dans datasets/.

ðŸ”¹ Caltech Pedestrian
Site : https://www.kaggle.com/datasets/kalvinquackenbush/caltechpedestriandataset

Dossier cible : datasets/Caltech/

Les scripts convertir_vbb.py, extract_images.py, extract_annotations.py
permettent de convertir .seq + .vbb â†’ images + labels YOLO.

ðŸ”¹ INRIA Person
Repo : https://www.kaggle.com/datasets/jcoral02/inriaperson

Dossier cible : datasets/INRIA/

ðŸ”¹ kitti_tracking 
Site : https://www.kaggle.com/datasets/leducnhuan/kitti-tracking

Dossier cible : datasets/kitti_tracking/

Une fois par exemple les archives kitti_tracking extraites, vous obtenez la structure officielle suivante :

```text
datasets/kitti_tracking/
    â””â”€â”€ training/
        â””â”€â”€ image_02/
            â”œâ”€â”€ 0000/
            â”œâ”€â”€ 0001/
            â”œâ”€â”€ 0012/
            â”œâ”€â”€ 0019/
            â””â”€â”€ ...
```

Lâ€™idÃ©e est simplement de placer les images de tracking dans datasets/kitti_tracking/...
en respectant lâ€™organisation native de kitti_tracking.

# 4. RÃ©sultats de dÃ©tection

Haar + SVM (Caltech - Caltech/ INRIA - INRIA)
<p align="center"> <img src="images/haar_caltech_1.png" width="260"/> <img src="images/haar_caltech_2.png" width="260"/> </p> <p align="center"> <img src="images/haar_inria_1.png" width="260"/> <img src="images/haar_inria_2.png" width="260"/> </p>
HOG + SVM (INRIA - INRIA)
<p align="center"> <img src="images/hog_inria_1.png" width="260"/> <img src="images/hog_inria_2.png" width="260"/> </p>
YOLOv8s (modÃ¨le entraÃ®nÃ© sur Caltech, testÃ© sur INRIA)
<p align="center"> <img src="images/yolo_caltech_inria_1.jpg" width="260"/> <img src="images/yolo_caltech_inria_2.jpg" width="260"/> </p>

# 5. RÃ©sultats de suivi
Les vidÃ©os finales de suivi sont dans :

[Voir la vidÃ©o Bytetrack](https://bizoffice4827-my.sharepoint.com/:v:/g/personal/technicien_mrp92_fr/IQAaIrXX4PK9TIhSjS7_q021AbbQRkKVM1pONkk9cDbHsRI?e=ok9DhC)

ou

[Voir la vidÃ©o DeepSort](https://bizoffice4827-my.sharepoint.com/:v:/g/personal/technicien_mrp92_fr/IQBHNRAXR811TrvHZ0TlRg96AX2TmrvCqu6MQ3_LM9kQlb8?e=KR0Vzy)


# 6. Environnement logiciel (Cluster Trilium)
Sur le cluster Trilium, avant dâ€™exÃ©cuter lâ€™entraÃ®nement ou les Ã©valuations YOLO,
les modules et bibliothÃ¨ques suivants sont chargÃ©s / installÃ©s :

module load python/3.11.5
module load gcc opencv/4.12.0 python script-stick

# activation de l'environnement virtuel (exemple)
source /chemin/vers/mon_env/bin/activate

# installation des dÃ©pendances
pip install --no-index \
  -f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic \
  pywavelets scikit-learn ultralytics

Ces commandes sont exÃ©cutÃ©es avant :

sbatch train_yolo.slurm

yolo detect val ...

yolo detect predict ...

yolo track ...

# 7. ModÃ¨le YOLOv8s (base + fine-tuning)
## 7.1 ModÃ¨le de base (prÃ©-entraÃ®nÃ© COCO)
Fichier : yolov8s.pt

TÃ©lÃ©chargement officiel :
https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt

Ce modÃ¨le est utilisÃ© dans train_yolo.slurm comme point de dÃ©part :

MODEL="yolov8s.pt"

## 7.2 ModÃ¨le final (fine-tunÃ© sur Caltech)
Le fine-tuning sur Caltech produit le meilleur modÃ¨le du projet (celui utilisÃ© dans lâ€™article) :

modeles/caltech_person/weights/best.pt
Ce modÃ¨le :

- est entraÃ®nÃ© sur Caltech (train),

- est Ã©valuÃ© sur Caltech (test) dans le script SLURM (yolo val split=test),

- est ensuite rÃ©utilisÃ© pour lâ€™Ã©valuation Caltech â†’ INRIA (cross-dataset),

- sert de modÃ¨le unique pour tous les tests et pour les deux trackers (DeepSORT et ByteTrack).

# 8. EntraÃ®nement YOLOv8s sur Caltech
Lâ€™entraÃ®nement se fait via le script SLURM : sbatch train_yolo.slurm

Dans ce script :

model=$MODEL pointe vers yolov8s.pt (modÃ¨le de base COCO),

data=config/data_caltech.yaml dÃ©crit les chemins du dataset Caltech converti en format YOLO,

les paramÃ¨tres par dÃ©faut (batch, epochs, etc.) sont ajustÃ©s pour le cluster.

Ã€ la fin de lâ€™entraÃ®nement, on valide  sur le split test de Caltech pour les performances officielles:

yolo detect val \
  model=modeles/caltech_person/weights/best.pt \
  data=config/data_caltech.yaml \
  split=test

Ce yolo val fournit les performances officielles Caltech â†’ Caltech
utilisÃ©es dans le rapport (mAP@50, F1, etc.).

Temps dâ€™exÃ©cution observÃ© sur Trilium :

~ 2 h 05 min 41 s sur 4 GPUs (H100).

# 9. Ã‰valuation YOLOv8s (cross-dataset Caltech â†’ INRIA)
AprÃ¨s lâ€™entraÃ®nement sur Caltech, on rÃ©utilise le mÃªme modÃ¨le :

yolo detect val \
  model=modeles/caltech_person/weights/best.pt \
  data=config/data_inria.yaml \
  split=test

RÃ©sultat principal (Caltech â†’ INRIA) :

mAP@50 â‰ˆ 0.689

F1 et PR dÃ©taillÃ©s dans lâ€™article (courbes PR/F1 + matrice de confusion): **Etude Comparative des techniques de Detection et Suivi de Pietons**

Dans cet article, câ€™est ce cas Caltech â†’ INRIA qui est considÃ©rÃ© comme
meilleur scÃ©nario global (modÃ¨le entraÃ®nÃ© sur un dataset plus difficile et testÃ© sur un plus simple).

# 10. Suivi multi-objets
## 10.1 DeepSORT (ReID MobileNet)

DeepSORT nâ€™est pas intÃ©grÃ© directement dans Ultralytics :
on utilise le script Python track_ReID_deepsort.py, qui prend en entrÃ©e :

les images kitti_tracking pour une sÃ©quence (ex. 0019),

les dÃ©tections YOLOv8s au format e (.txt) gÃ©nÃ©rÃ©es par Ultralytics,

un dossier de sortie pour les frames annotÃ©es et les labels avec ID.

### 10.1.1 GÃ©nÃ©rer les dÃ©tections YOLO sur kitti_tracking

yolo detect predict \
  model=modeles/caltech_person/weights/best.pt \
  source=datasets/kitti_tracking/training/image_02/0019 \
  imgsz=1408 \
  conf=0.60 \
  save=True \
  save_txt=True \
  project=runs/detect \
  name=kitti_tracking_0019_yolo

Cela produit une structure de ce type :

```text
runs/detect/kitti_tracking_0019_yolo/
 â”œâ”€â”€ 000000.png
 â”œâ”€â”€ 000001.png
 â”œâ”€â”€ ...
 â””â”€â”€ labels/
      â”œâ”€â”€ 000000.txt    # cls cx cy w h conf
      â”œâ”€â”€ 000001.txt
      â””â”€â”€ ...

```

### 10.1.2 Lancer DeepSORT


python scripts/track_ReID_deepsort.py \
  --img_dir  datasets/kitti_tracking/training/image_02/0019 \
  --dets_dir runs/detect/kitti_tracking_0019_yolo/labels \
  --out_dir  runs/tracking/deepsort_0019 \
  --embedder mobilenet \
  --max_age 10 \
  --n_init 3 \
  --max_cosine_distance 0.4

ParamÃ¨tres principaux :

--img_dir : images kitti_tracking dâ€™une sÃ©quence (ex. 0019)

--dets_dir : fichiers .txt YOLO gÃ©nÃ©rÃ©s par yolo detect predict

--out_dir : dossier de sortie des rÃ©sultats DeepSORT

--embedder : modÃ¨le ReID utilisÃ© (mobilenet)

--max_age : durÃ©e de vie dâ€™une piste sans dÃ©tection

--n_init : nombre de frames nÃ©cessaires pour valider une piste

--max_cosine_distance : seuil dâ€™acceptation pour la similaritÃ© dâ€™apparence

RÃ©sultats :

```text
runs/tracking/deepsort_0019/
 â”œâ”€â”€ frames/
 â”‚    â”œâ”€â”€ 000000.png      # image annotÃ©e (bbox + ID)
 â”‚    â”œâ”€â”€ 000001.png
 â”‚    â””â”€â”€ ...
 â””â”€â”€ labels/
 â”‚    â”œâ”€â”€ 000000.txt      # cls cx cy w h track_id
 â”‚    â”œâ”€â”€ 000001.txt
 â”‚    â””â”€â”€ ...
 â””â”€â”€ deepsort_0019.mp4 # vidÃ©o finale
```

Les vidÃ©os finales visibles dans videos/deepsort.mp4 sont construites
Ã  partir de ces frames via images_to_videos.py ;

python scripts/images_to_videos.py \
  --input_dir runs/tracking/deepsort_0019/frames \
  --output_path runs/tracking/deepsort_0019/videos/deepsort_0019.mp4 \
  --fps 10

ParamÃ¨tres :
	â€“ input_dir : dossier contenant les images annotÃ©es
	â€“ output_path : nom de la vidÃ©o gÃ©nÃ©rÃ©e
	â€“ fps : nombre dâ€™images par seconde (10 car pas assez d'image pour kitti_tracking)

## 10.2 ByteTrack (Ultralytics)

ByteTrack est intÃ©grÃ© dans Ultralytics via `yolo track`.  
Dans notre workflow, comme pour DeepSORT, `yolo track` gÃ©nÃ¨re **des images annotÃ©es**  
et **non pas directement une vidÃ©o**.  
La vidÃ©o finale est construite ensuite avec `images_to_videos.py`.

### 10.2.1 GÃ©nÃ©rer les images ByteTrack

yolo track \
  model="modeles/caltech_person/weights/best.pt" \
  source="datasets/kitti_tracking/training/image_02/0019" \
  imgsz=1408 \
  conf=0.60 \
  tracker="bytetrack.yaml" \
  save=True \
  save_txt=True \
  save_json=True \
  project="runs/tracking" \
  name="bytetrack_0019/frames"

model= : modÃ¨le YOLOv8s fine-tunÃ© sur Caltech

source= : dossier dâ€™images kitti_tracking pour une sÃ©quence

tracker="bytetrack.yaml" : active ByteTrack

save=True : enregistre automatiquement

save_txt=True : enregistre les labels avec track_id

save_json=True : exporte les rÃ©sultats en JSON (format MOT-compatible)

Sorties typiques :

```text

runs/tracking/bytetrack_0019
 â”œâ”€â”€ frames/       
 â”‚    â”œâ”€â”€ 000000.png      # image annotÃ©e (bbox + ID)
 â”‚    â”œâ”€â”€ 000001.png
 â”‚    â””â”€â”€ ...
 â”œâ”€â”€ labels/
 â”‚    â”œâ”€â”€ 000000.txt           # cls cx cy w h track_id
 â”‚    â”œâ”€â”€ 000001.txt
 â”‚    â””â”€â”€ ...
 â””â”€â”€ predictions.json          # rÃ©sultats pour Ã©valuation MOT

```

### 10.2.2 Construire la vidÃ©o ByteTrack

Comme pour DeepSORT, les frames gÃ©nÃ©rÃ©es sont converties en vidÃ©o :

python scripts/images_to_videos.py \
  --input_dir runs/tracking/bytetrack_0019/frames \
  --output_path runs/tracking/bytetrack_0019/videos/bytetrack_0019.mp4 \
  --fps 10


ParamÃ¨tres :

input_dir : dossier contenant les images annotÃ©es

output_path : vidÃ©o finale

fps : 10 (adÃ©quat pour KITTI car peu d'images)

La vidÃ©o obtenue est ensuite copiÃ©e dans : videos/ByteTrack.mp4

Ces fichiers (labels) peuvent ensuite Ãªtre convertis et Ã©valuÃ©s avec :

scripts/convert_Pred_to_MOT.py

scripts/eval_MOT.py

pour obtenir les mÃ©triques IDF1, MOTA, etc., comme dans lâ€™article.

# 11. ReproductibilitÃ© (rÃ©sumÃ©)

- Charger lâ€™environnement Trilium (Section 6)

- TÃ©lÃ©charger et placer les datasets (Section 3)

- Convertir Caltech en images + YOLO (scripts convertir_vbb.py, extract_images.py, extract_annotations.py)

- GÃ©nÃ©rer les splits :
   config/liste_chemin_image.sh  (mais sera gÃ©rer par le slurm ci-dessous donc ignorer cette Ã©tape)

- EntraÃ®ner YOLOv8s sur Caltech :

- sbatch train_yolo.slurm
- dos2unix train_yolo.slurm

 â†’ modÃ¨le : modeles/caltech_person/weights/best.pt
 â†’ validation automatique Caltech â†’ Caltech (yolo val split=test)

- Ã‰valuer Caltech â†’ INRIA :

 yolo detect val \
   model=modeles/caltech_person/weights/best.pt \
   data=config/data_inria.yaml \
   split=test

- GÃ©nÃ©rer les dÃ©tections kitti_tracking (pour le tracking) avec yolo detect predict.

- Lancer DeepSORT avec track_ReID_deepsort.py.

- Lancer ByteTrack avec yolo track ... tracker="bytetrack.yaml".

# 12. ModÃ¨le final du projet
Le modÃ¨le unique utilisÃ© dans tous les rÃ©sultats de lâ€™article est : modeles/caltech_person/weights/best.pt
- entraÃ®nÃ© sur Caltech

- Ã©valuÃ© sur Caltech (officiel) via yolo val split=test

- testÃ© en cross-dataset Caltech â†’ INRIA (meilleure configuration)

- utilisÃ© pour DeepSORT et ByteTrack sur kitti_tracking.

---

# Contact

Pour toute question concernant le projet, vous pouvez contacter :

**Ousmane MAIGA**  
**eom6713@umoncton.ca**

